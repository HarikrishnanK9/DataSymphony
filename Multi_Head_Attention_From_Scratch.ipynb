{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPbnrvQLuhA1sf2V0dHNror",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarikrishnanK9/DataSymphony/blob/main/Multi_Head_Attention_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3xuiK1fjIXbB"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention:\n",
        "  \"\"\"\n",
        "  Multi-head attention.\n",
        "  Parameters:\n",
        "  num_hiddens: int\n",
        "  Number of hidden units.\n",
        "  num_heads: int\n",
        "  Number of attention heads.\n",
        "  dropout: float\n",
        "  Dropout rate.\n",
        "  bias: bool\n",
        "  Whether to include bias parameters in the model.\n",
        "  \"\"\"\n",
        "  def __init__(self, num_hiddens, num_heads, dropout=0.0, bias=False):\n",
        "        self.num_heads = num_heads\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.d_k = self.d_v = num_hiddens // num_heads\n",
        "        self.W_q = np.random.rand(num_hiddens, num_hiddens)\n",
        "        self.W_k = np.random.rand(num_hiddens, num_hiddens)\n",
        "        self.W_v = np.random.rand(num_hiddens, num_hiddens)\n",
        "        self.W_o = np.random.rand(num_hiddens, num_hiddens)\n",
        "        if bias:\n",
        "          self.b_q = np.random.rand(num_hiddens)\n",
        "          self.b_k = np.random.rand(num_hiddens)\n",
        "          self.b_v = np.random.rand(num_hiddens)\n",
        "          self.b_o = np.random.rand(num_hiddens)\n",
        "        else:\n",
        "          self.b_q = self.b_k = self.b_v = self.b_o = np.zeros(num_hiddens)\n",
        "\n",
        "  def transpose_qkv(self, X):\n",
        "        \"\"\"\n",
        "        Transposition for batch processing\n",
        "        Parameters:\n",
        "        X: np.ndarray\n",
        "        Input tensor\n",
        "        Returns:\n",
        "        np\n",
        "        Transposed tensor\n",
        "        \"\"\"\n",
        "        X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
        "        X = X.transpose(0, 2, 1, 3)\n",
        "        return X.reshape(-1, X.shape[2], X.shape[3])\n",
        "  def transpose_output(self, X):\n",
        "        \"\"\"\n",
        "        Transposition for output\n",
        "        Parameters:\n",
        "        X: np.ndarray\n",
        "        Input tensor\n",
        "        Returns:\n",
        "        np\n",
        "        Transposed tensor\n",
        "        \"\"\"\n",
        "        X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
        "        X = X.transpose(0, 2, 1, 3)\n",
        "        return X.reshape(X.shape[0], X.shape[1], -1)\n",
        "  def scaled_dot_product_attention(self, Q, K, V, valid_lens):\n",
        "        \"\"\"\n",
        "        Scaled dot product attention\n",
        "        Parameters:\n",
        "        Q: np.ndarray\n",
        "        Query tensor\n",
        "        K: np.ndarray\n",
        "        Key tensor\n",
        "        V: np.ndarray\n",
        "        Value tensor\n",
        "        valid_lens: np.ndarray\n",
        "        Valid lengths for the query\n",
        "        Returns:\n",
        "        np\n",
        "        Output tensor\n",
        "        \"\"\"\n",
        "        d_k = Q.shape[-1]\n",
        "        scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n",
        "        if valid_lens is not None:\n",
        "          mask = np.arange(scores.shape[-1]) < valid_lens[:, None]\n",
        "          scores = np.where(mask[:, None, :], scores, -np.inf)\n",
        "          attention_weights = np.exp(scores - np.max(scores, axis=-1,keepdims=True))\n",
        "          attention_weights /= attention_weights.sum(axis=-1, keepdims=True)\n",
        "        return np.matmul(attention_weights, V)\n",
        "  def forward(self, queries, keys, values, valid_lens):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "        Parameters:\n",
        "        queries: np.ndarray\n",
        "        Query tensor\n",
        "        keys: np.ndarray\n",
        "        Key tensor\n",
        "        values: np.ndarray\n",
        "        Value tensor\n",
        "        valid_lens: np.ndarray\n",
        "        Valid lengths for the query\n",
        "        Returns:\n",
        "        np\n",
        "        Output tensor\n",
        "        \"\"\"\n",
        "        queries = self.transpose_qkv(np.dot(queries, self.W_q) + self.b_q)\n",
        "        keys = self.transpose_qkv(np.dot(keys, self.W_k) + self.b_k)\n",
        "        values = self.transpose_qkv(np.dot(values, self.W_v) + self.b_v)\n",
        "        if valid_lens is not None:\n",
        "          valid_lens = np.repeat(valid_lens, self.num_heads, axis=0)\n",
        "          output = self.scaled_dot_product_attention(queries, keys, values,valid_lens)\n",
        "          output_concat = self.transpose_output(output)\n",
        "        return np.dot(output_concat, self.W_o) + self.b_o"
      ],
      "metadata": {
        "id": "ecQZ5-frL5_S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "wHlikfi3NKxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dimensions and initialize multi-head attention\n",
        "num_hiddens, num_heads = 100, 5\n",
        "attention = MultiHeadAttention(num_hiddens, num_heads, dropout=0.5, bias=False)\n",
        "# Define sample data\n",
        "batch_size, num_queries, num_kvpairs = 2, 4, 6\n",
        "valid_lens = np.array([3, 2])\n",
        "print(valid_lens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr5K-tvTM4br",
        "outputId": "c04385e1-dfdf-4869-878c-024bf3b2e525"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.rand(batch_size, num_queries, num_hiddens) # Use random data to simulate input queries\n",
        "Y = np.random.rand(batch_size, num_kvpairs, num_hiddens) # Use random data to simulate key-value pairs\n",
        "print(\"Query data shape:\", X.shape)\n",
        "print(\"Key-value data shape:\", Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqdFRnHPNZT-",
        "outputId": "547a4b14-6e78-4fe9-d930-10895ef62083"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query data shape: (2, 4, 100)\n",
            "Key-value data shape: (2, 6, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply multi-head attention\n",
        "output = attention.forward(X, Y, Y, valid_lens)\n",
        "print(\"Output shape:\", output.shape) # Expected shape: (batch_size,num_queries, num_hiddens)\n",
        "# Output sample data\n",
        "print(output[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diBumJZ9Nbnm",
        "outputId": "a694f759-7044-4140-864e-68a98cf5f227"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (2, 4, 100)\n",
            "[1352.90077426 1253.23139856 1556.6128199  1513.80032417 1305.43769393\n",
            " 1365.82303938 1512.2497699  1529.52131061 1512.74313702 1443.56795073\n",
            " 1451.7070232  1446.87191905 1422.55158685 1370.9700987  1256.41455512\n",
            " 1335.76729195 1367.02833798 1409.54372477 1550.95294193 1262.36758339\n",
            " 1387.6066865  1338.37699346 1515.92284845 1458.18847239 1478.65040157\n",
            " 1306.09947677 1291.69807397 1303.39347217 1295.94267472 1416.55255069\n",
            " 1257.38662215 1257.33406508 1393.12622816 1428.17794078 1302.9198136\n",
            " 1364.64700271 1443.89411916 1426.6393788  1367.66520473 1309.34922966\n",
            " 1483.18764286 1383.47380866 1441.52139262 1405.62452993 1231.76690432\n",
            " 1345.83482005 1357.48128796 1297.79420729 1421.10264642 1390.83237925\n",
            " 1478.86977488 1347.31935295 1500.22107762 1594.54308112 1588.83641395\n",
            " 1336.54909021 1390.43472962 1305.64747901 1531.57811718 1287.63190405\n",
            " 1372.14942575 1290.28548264 1487.52634649 1313.54870997 1341.55787612\n",
            " 1343.99477134 1451.44780452 1483.40890087 1453.64123604 1360.02780707\n",
            " 1393.00301648 1411.35716884 1430.50344918 1406.19175465 1456.62172202\n",
            " 1474.05651243 1354.08640238 1331.80268707 1406.3710255  1396.98096226\n",
            " 1375.92761928 1344.91271369 1477.22926264 1387.9425852  1374.37223077\n",
            " 1302.81235383 1512.01087057 1312.4635397  1451.63134176 1448.31042146\n",
            " 1432.60230032 1425.40982064 1361.92527282 1453.3622247  1571.65733419\n",
            " 1332.62989251 1419.64147048 1651.75297312 1359.43916653 1615.29372501]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8WOuckOQNphI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}